{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLdNJO45VHTp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import SimpleRNN, GRU, LSTM, Embedding, Layer, Dense, Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import tensorflow.keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBfxgvHYVIG9",
        "outputId": "37e4a871-e4ec-4cbb-d2aa-2c7bdaa3ce0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwTiZajBTzTN"
      },
      "outputs": [],
      "source": [
        "def data_process(directory, tokenize_english=None, tokenize_tamil=None):\n",
        "#This function creates textual data to model readable one\n",
        "    df = pd.read_csv(directory, sep=\"\\t\", header=None)\n",
        "\n",
        "    for column in [0,1]:\n",
        "      df[column]=\"\\t\"+df[column]+\"\\n\"\n",
        "\n",
        "    if tokenize_english is None:\n",
        "        tokenize_english = Tokenizer(char_level=True)\n",
        "        tokenize_english.fit_on_texts(df[1].astype(str).tolist())\n",
        "\n",
        "    input_lang_tensor = tokenize_english.texts_to_sequences(df[1].astype(str).tolist())\n",
        "    input_lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_lang_tensor, padding='post')\n",
        "\n",
        "    if tokenize_tamil is None:\n",
        "        tokenize_tamil = Tokenizer(char_level=True)\n",
        "        tokenize_tamil.fit_on_texts(df[0].astype(str).tolist())\n",
        "\n",
        "    targ_lang_tensor = tokenize_tamil.texts_to_sequences(df[0].astype(str).tolist())\n",
        "    targ_lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(targ_lang_tensor, padding='post')\n",
        "\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_lang_tensor, targ_lang_tensor))\n",
        "    dataset = dataset.shuffle(len(dataset))\n",
        "    \n",
        "    return dataset, tokenize_english, tokenize_tamil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIYvCbfuT4iW"
      },
      "outputs": [],
      "source": [
        "def layer_selection(neuron, dropout, layer_name, return_sequences=False, return_state=False):\n",
        "\n",
        "    if layer_name==\"SimpleRNN\":\n",
        "        return SimpleRNN(units=neuron, dropout=dropout, return_sequences=return_sequences, return_state=return_state)\n",
        "\n",
        "    elif layer_name==\"GRU\":\n",
        "        return GRU(units=neuron, dropout=dropout, return_sequences=return_sequences, return_state=return_state)\n",
        "\n",
        "    elif layer_name==\"LSTM\":\n",
        "        return LSTM(units=neuron, dropout=dropout, return_sequences=return_sequences, return_state=return_state)\n",
        "\n",
        "class Attention(Layer):\n",
        "#This class is used to have attention on top of the encoder  \n",
        "  def __init__(self, neuron):\n",
        "    super(Attention, self).__init__()\n",
        "    self.Q = Dense(neuron)\n",
        "    self.k = Dense(neuron)\n",
        "    self.VT = Dense(1)\n",
        "# This function uses Dot product attention formula\n",
        "  def call(self, encoder_state, encoder_out):    \n",
        "    encoder_state = tf.concat(encoder_state, 1)\n",
        "    encoder_state = tf.expand_dims(encoder_state, 1)\n",
        "    score = self.VT(tf.nn.tanh(self.Q(encoder_state) + self.k(encoder_out)))\n",
        "    att_weights = tf.nn.softmax(score, axis=1)\n",
        "    context = att_weights * encoder_out\n",
        "    context = tf.reduce_sum(context, axis=1)\n",
        "    return context, att_weights\n",
        "\n",
        "class TranslationEncoder(tf.keras.Model):\n",
        "    def __init__(self, emb_dimension, neuron, dropout, layer, no_of_layers, encoder_word_size):\n",
        "        super(TranslationEncoder, self).__init__()\n",
        "        self.neuron = neuron\n",
        "        self.dropout = dropout\n",
        "        self.layer = layer\n",
        "        self.no_of_layers = no_of_layers\n",
        "        self.embedding = Embedding(encoder_word_size, emb_dimension)\n",
        "        self.create_layers()\n",
        "\n",
        "    def call(self, statex, hidden):\n",
        "        statex = self.embedding(statex)\n",
        "        statex = self.netwrk_layers[0](statex, initial_state=hidden)\n",
        "\n",
        "        for layer in self.netwrk_layers[1:]:\n",
        "            statex = layer(statex)\n",
        "\n",
        "        state_output, state = statex[0], statex[1:]\n",
        "\n",
        "        return state_output, state\n",
        "    \n",
        "    def create_layers(self):\n",
        "        self.netwrk_layers = []\n",
        "\n",
        "        for i in range(self.no_of_layers):\n",
        "            self.netwrk_layers.append(layer_selection(self.neuron, self.dropout, self.layer, return_sequences=True, return_state=True))\n",
        "\n",
        "\n",
        "    def build_hidden_state(self, batch_size):\n",
        "        h= [tf.zeros((batch_size, self.neuron))]\n",
        "        if self.layer == \"LSTM\":\n",
        "          h=h*2\n",
        "        return h  \n",
        "\n",
        "\n",
        "class TransaltionDecoder(tf.keras.Model):\n",
        "    def __init__(self, emb_dimension, neuron, dropout, layer, no_of_layers, decoder_word_size, att=False):\n",
        "        super(TransaltionDecoder, self).__init__()\n",
        "        self.neuron = neuron\n",
        "        self.dropout = dropout\n",
        "        self.layer = layer\n",
        "        self.no_of_layers = no_of_layers\n",
        "        self.embedding_layer = Embedding(input_dim=decoder_word_size, output_dim=emb_dimension)\n",
        "        self.att = att\n",
        "        \n",
        "        self.dense = Dense(decoder_word_size, activation=\"softmax\")\n",
        "        self.flatten = Flatten()\n",
        "        if self.att:\n",
        "            self.att_layer = Attention(self.neuron)\n",
        "        self.create_layers()\n",
        "\n",
        "    def call(self, h, hidden, encoder_out=None):\n",
        "        \n",
        "        h = self.embedding_layer(h)\n",
        "\n",
        "        if self.att:\n",
        "            context, att_weights = self.att_layer(hidden, encoder_out)\n",
        "            h = tf.concat([tf.expand_dims(context, 1), h], -1)\n",
        "        else:\n",
        "            att_weights = None\n",
        "\n",
        "        h = self.netwrk_layers[0](h, initial_state=hidden)\n",
        "\n",
        "        for layer in self.netwrk_layers[1:]:\n",
        "            h = layer(h)\n",
        "\n",
        "        state_output, state = h[0], h[1:]\n",
        "\n",
        "        state_output = self.dense(self.flatten(state_output))\n",
        "        \n",
        "        return state_output, state, att_weights\n",
        "\n",
        "    def create_layers(self):\n",
        "        self.netwrk_layers = []    \n",
        "\n",
        "        for i in range(self.no_of_layers - 1):\n",
        "            self.netwrk_layers.append(layer_selection(self.neuron, self.dropout, self.layer, return_sequences=True, return_state=True))\n",
        "        \n",
        "        self.netwrk_layers.append(layer_selection(self.neuron, self.dropout, self.layer, return_sequences=False, return_state=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ345VvBUVa-"
      },
      "outputs": [],
      "source": [
        "class NLPModel():\n",
        "    def __init__(self, emb_dimension, neuron, dropout, no_of_layers, layer, att=False):\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.no_of_layers = no_of_layers\n",
        "        self.layer = layer\n",
        "        self.neuron = neuron\n",
        "        self.dropout = dropout\n",
        "        self.att = att\n",
        "        self.batch_size = 64\n",
        "\n",
        "    def initialize(self, input_lang_tokenizer, targ_lang_tokenizer, loss, optimizer, metric):\n",
        "        self.input_lang_tokenizer = input_lang_tokenizer\n",
        "        self.targ_lang_tokenizer = targ_lang_tokenizer\n",
        "        encoder_word_size = len(self.input_lang_tokenizer.word_index) + 1\n",
        "        decoder_word_size = len(self.targ_lang_tokenizer.word_index) + 1\n",
        "        self.encoder = TranslationEncoder(self.emb_dimension, self.neuron, self.dropout, self.layer, self.no_of_layers, encoder_word_size)\n",
        "        self.decoder = TransaltionDecoder(self.emb_dimension, self.neuron, self.dropout, self.layer, self.no_of_layers, decoder_word_size, self.att)    \n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.metric = metric\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def training(self, english, tamil, encoder_state):\n",
        "        loss = 0 \n",
        "        # This function is to do training step by step in each epoch\n",
        "        with tf.GradientTape() as t: \n",
        "            encoder_out, encoder_state = self.encoder(english, encoder_state)\n",
        "            decoder_state = encoder_state\n",
        "            decoder_input = tf.expand_dims([self.targ_lang_tokenizer.word_index[\"\\t\"]]*self.batch_size ,1)\n",
        "\n",
        "            if random.random() < self.teacher_forcing:\n",
        "                for i in range(1, tamil.shape[1]):\n",
        "\n",
        "                    predictions, decoder_state, _ = self.decoder(decoder_input, decoder_state, encoder_out)\n",
        "                    loss = loss+self.loss(tamil[:,i], predictions)\n",
        "                    self.metric.update_state(tamil[:,i], predictions)\n",
        "                    decoder_input = tf.expand_dims(tamil[:,i], 1)\n",
        "            \n",
        "            else:\n",
        "\n",
        "                for i in range(1, tamil.shape[1]):\n",
        "\n",
        "                    predictions, decoder_state, _ = self.decoder(decoder_input, decoder_state, encoder_out)\n",
        "                    loss = loss+self.loss(tamil[:,i], predictions)\n",
        "                    self.metric.update_state(tamil[:,i], predictions)\n",
        "\n",
        "                    predictions = tf.argmax(predictions, 1)\n",
        "                    decoder_input = tf.expand_dims(predictions, 1)\n",
        "\n",
        "\n",
        "            total_batch_loss = loss / tamil.shape[1]\n",
        "\n",
        "            total_variables = self.encoder.variables + self.decoder.variables\n",
        "            final_gradients = t.gradient(loss, variables)\n",
        "\n",
        "            self.optimizer.apply_gradients(zip(final_gradients, total_variables))\n",
        "\n",
        "        return total_batch_loss, self.metric.result()\n",
        "\n",
        "    @tf.function\n",
        "    def validation(self, english, tamil, encoder_state):\n",
        "\n",
        "        loss = 0\n",
        "        # This function is to do validation step by step in each epoch  \n",
        "        encoder_out, encoder_state = self.encoder(english, encoder_state)\n",
        "        decoder_state = encoder_state\n",
        "        decoder_input = tf.expand_dims([self.targ_lang_tokenizer.word_index[\"\\t\"]]*self.batch_size ,1)\n",
        "        for t in range(1, tamil.shape[1]):\n",
        "            predictions, decoder_state, _ = self.decoder(decoder_input, decoder_state, encoder_out)\n",
        "            loss = loss+self.loss(tamil[:,t], predictions)\n",
        "            self.metric.update_state(tamil[:,t], predictions)\n",
        "            predictions = tf.argmax(predictions, 1)\n",
        "            decoder_input = tf.expand_dims(predictions, 1)\n",
        "        total_batch_loss = loss / tamil.shape[1]\n",
        "        \n",
        "        return total_batch_loss, self.metric.result()\n",
        "\n",
        "\n",
        "    def fit_model(self, dataset, v_data, batch_size=64, epochs=20, teacher_forcing=0.9):\n",
        "# This function uses teacher forcing technique before decoder      \n",
        "        self.teacher_forcing = teacher_forcing\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        steps = len(dataset) // self.batch_size\n",
        "        steps_in_validation = len(v_data) // self.batch_size\n",
        "        \n",
        "        dataset = dataset.batch(self.batch_size, drop_remainder=True)\n",
        "        v_data = v_data.batch(self.batch_size, drop_remainder=True)\n",
        "\n",
        "        english_sample, tamil_sample = next(iter(dataset))\n",
        "        self.max_tamil_length = tamil_sample.shape[1]\n",
        "        self.max_english_length = english_sample.shape[1]\n",
        "        for epoch in range(1, epochs+1):\n",
        "            loss_sum = 0\n",
        "            accuracy_sum = 0\n",
        "            self.metric.reset_states()\n",
        "            encoder_state = self.encoder.build_hidden_state(self.batch_size)\n",
        "            #Batch wise training happens in this loop\n",
        "            for batch, (english, tamil) in enumerate(dataset.take(steps)):\n",
        "                batch_loss, acc = self.training(english, tamil, encoder_state)\n",
        "                accuracy_sum = accuracy_sum+acc                \n",
        "                loss_sum =loss_sum+ batch_loss\n",
        "\n",
        "            average_accuracy = (accuracy_sum/steps)*100\n",
        "            average_loss = loss_sum/steps\n",
        "\n",
        "            total_validation_loss = 0\n",
        "            total_validation_accuracy = 0\n",
        "            self.metric.reset_states()\n",
        "            encoder_state = self.encoder.build_hidden_state(self.batch_size)\n",
        "            #Validation happens in this loop\n",
        "            for batch, (english, tamil) in enumerate(v_data.take(steps_in_validation)):\n",
        "                batch_loss, accuracy_v = self.validation(english, tamil, encoder_state)\n",
        "                total_validation_loss = total_validation_loss+batch_loss\n",
        "                total_validation_accuracy = total_validation_accuracy+accuracy_v\n",
        "\n",
        "            average_validation_accuracy = (total_validation_accuracy / steps_in_validation)*100\n",
        "            average_validation_loss = total_validation_loss / steps_in_validation\n",
        "            wandb.log({\"epoch\": epoch,\"training_loss\": average_loss,\"validation_loss\": average_validation_loss,\"training_accuracy\": average_accuracy,\"validation_accuracy\": average_validation_accuracy})\n",
        "\n",
        "\n",
        "    def stats(self, test_data, batch_size=100):\n",
        "        #This function computes test accuracy and loss\n",
        "        self.batch_size = batch_size\n",
        "        steps_in_test = len(test_data) // batch_size\n",
        "        test_data = test_data.batch(batch_size, drop_remainder=True) \n",
        "        test_loss_sum = 0\n",
        "        test_accuracy_sum = 0\n",
        "        self.metric.reset_states()\n",
        "        encoder_state = self.encoder.build_hidden_state(self.batch_size)\n",
        "        for batch, (english, tamil) in enumerate(test_data.take(steps_in_test)):\n",
        "            batch_loss, accuracy = self.validation(english, tamil, encoder_state)\n",
        "            test_loss_sum = test_loss_sum+batch_loss\n",
        "            test_accuracy_sum = test_accuracy_sum+accuracy \n",
        "        average_test_accuracy = test_accuracy_sum / steps_in_test\n",
        "        average_test_loss = test_loss_sum / steps_in_test\n",
        "    \n",
        "        return average_test_loss, average_test_accuracy\n",
        "\n",
        "\n",
        "    def word_translation(self, english_word):\n",
        "\n",
        "        english_word = \"\\t\"+english_word+\"\\n\"\n",
        "        english = self.input_lang_tokenizer.texts_to_sequences([english_word])\n",
        "        english = tf.keras.preprocessing.sequence.pad_sequences(english, maxlen=self.max_english_length, padding=\"post\")\n",
        "        final_prediction = \"\"\n",
        "\n",
        "        encoder_state = self.encoder.build_hidden_state(1)\n",
        "        encoder_out, encoder_state = self.encoder(english, encoder_state)\n",
        "        decoder_state = encoder_state\n",
        "        decoder_input = tf.expand_dims([self.targ_lang_tokenizer.word_index[\"\\t\"]]*1, 1)\n",
        "\n",
        "        for t in range(1, self.max_tamil_length):\n",
        "            pred, decoder_state, _ = self.decoder(decoder_input, decoder_state, encoder_out)                     \n",
        "            pred = tf.argmax(pred, 1)\n",
        "            next_character = self.targ_lang_tokenizer.index_word[pred.numpy().item()]\n",
        "            final_prediction = final_prediction+next_character\n",
        "            decoder_input = tf.expand_dims(pred, 1)\n",
        "\n",
        "            if next_character == \"\\n\":\n",
        "                return final_prediction[:-1]\n",
        "\n",
        "        return final_prediction[:-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrQSYWEi-aBw",
        "outputId": "f20cec23-1d88-4f18-d45c-5f2f91ed7596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 37.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=af2d31901d3c8991c43c37cee4673dd47ddf6fe277206733e3b564c97993b36b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "wandb.init()"
      ],
      "metadata": {
        "id": "vIpBKx49cgOI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "bb0d9624-e952-43e4-b2a3-b01b4a31b7bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtejoram\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220513_192625-9rt2np87</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/tejoram/uncategorized/runs/9rt2np87\" target=\"_blank\">earnest-sea-25</a></strong> to <a href=\"https://wandb.ai/tejoram/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tejoram/uncategorized/runs/9rt2np87?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7efd6fe7c890>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    config_defaults = {\"emb_dimension\": 64, \n",
        "                       \"neuron\": 128,\n",
        "                       \"dropout\": 0,                       \n",
        "                       \"no_of_layers\": 1,\n",
        "                       \"layer\": \"LSTM\",\n",
        "                       \"att\": False,\n",
        "                       \"teacher_forcing\": 1.0\n",
        "                       }\n",
        "\n",
        "    wandb.init(project=\"cs6910-assignment3\", entity=\"tejoram\")\n",
        "    config=config_defaults\n",
        "    emb_dimension=wandb.config.emb_dimension\n",
        "    neuron=wandb.config.neuron\n",
        "    dropout=wandb.config.dropout    \n",
        "    no_of_layers=wandb.config.no_of_layers\n",
        "    layer=wandb.config.layer\n",
        "    att=wandb.config.att                       \n",
        "    teacher_forcing= wandb.config.teacher_forcing\n",
        "    wandb.run.name = \"emd_{}_u_{}_d_{}_No.l_{}_l.type_{}_at_{}_tf_{}\".format(emb_dimension, \\\n",
        "                                                                             neuron , \\\n",
        "                                                                             dropout , \\\n",
        "                                                                             no_of_layers , \\\n",
        "                                                                             layer , \\\n",
        "                                                                             att , \\\n",
        "                                                                             teacher_forcing)\n",
        "\n",
        "    \n",
        "    train_data = \"/content/drive/MyDrive/Assignment3/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\"\n",
        "    validation_data= \"/content/drive/MyDrive/Assignment3/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\"\n",
        "    dataset, input_lang_tokenizer, targ_lang_tokenizer = data_process(train_data)\n",
        "    v_data, _, _ = data_process(validation_data, input_lang_tokenizer, targ_lang_tokenizer)\n",
        "    model = NLPModel(emb_dimension, neuron, dropout, no_of_layers, layer, att)\n",
        "    model.initialize(input_lang_tokenizer, targ_lang_tokenizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metric = tf.keras.metrics.SparseCategoricalAccuracy())\n",
        "    model.fit_model(dataset, v_data, epochs=15, teacher_forcing=wandb.config.teacher_forcing)\n",
        "\n"
      ],
      "metadata": {
        "id": "Em9b4QDDd8z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "  'metric': {'name': 'validation_accuracy','goal':'maximize'},    \n",
        "  \"name\": \"Assignment3\",\n",
        "  \"method\": \"grid\",\n",
        "  \"parameters\": {\n",
        "        \"emb_dimension\": {\n",
        "            \"values\": [64]\n",
        "        },      \n",
        "        \"neuron\": {\n",
        "            \"values\": [256]\n",
        "        },     \n",
        "        \"dropout\": {\n",
        "            \"values\": [0.2]\n",
        "        },         \n",
        "        \"no_of_layers\": {\n",
        "           \"values\": [3]\n",
        "        },\n",
        "        \"layer\": {\n",
        "            \"values\": [\"LSTM\"]\n",
        "        },\n",
        "        \"att\": {\n",
        "            \"values\": [False]\n",
        "        },        \n",
        "         \"teacher_forcing\": {\n",
        "            \"values\": [1.0]\n",
        "        }      \n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "G-oUvJVpppKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"Assignment3\")"
      ],
      "metadata": {
        "id": "mqtMuX8DptAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ae84d4-b769-4aa8-c9f3-9f209d462e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 6ltjhljk\n",
            "Sweep URL: https://wandb.ai/tejoram/Assignment3/sweeps/6ltjhljk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, function=train)"
      ],
      "metadata": {
        "id": "wEM2830KBdBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlw8ZA-cWB9i"
      },
      "outputs": [],
      "source": [
        "def besttest(emb_dimension, neuron, dropout, no_of_layers, layer, att, teacher_forcing=1.0):\n",
        "    \n",
        "    train_data= \"/content/drive/MyDrive/Assignment3/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\"\n",
        "    validation_data= \"/content/drive/MyDrive/Assignment3/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\"\n",
        "    test_data= \"/content/drive/MyDrive/Assignment3/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\"\n",
        "\n",
        "\n",
        "    model = NLPModel(emb_dimension, neuron, dropout, no_of_layers, layer, att)\n",
        "    dataset, input_lang_tokenizer, targ_lang_tokenizer = data_process(train_data)\n",
        "    v_data, _, _ = data_process(validation_data, input_lang_tokenizer, targ_lang_tokenizer)\n",
        "    model.initialize(input_lang_tokenizer, targ_lang_tokenizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metric = tf.keras.metrics.SparseCategoricalAccuracy())\n",
        "    model.fit_model(dataset, v_data, epochs=20, teacher_forcing=teacher_forcing)\n",
        "\n",
        "    test_data_processed, _, _ = data_process(test_data, model.input_lang_tokenizer, model.targ_lang_tokenizer)\n",
        "    test_loss, test_accuarcy = model.stats(test_data_processed)\n",
        "    print(\"\\ncharacter level accuracy:\",test_accuarcy)\n",
        "    print(\"\\ncharacter level loss:\",test_loss)\n",
        "\n",
        "    td = pd.read_csv(test_data, sep=\"\\t\", header=None)\n",
        "    tamil = td[0].astype(str).tolist()\n",
        "    english = td[1].astype(str).tolist() \n",
        "    predictions = []\n",
        "    for english_word in english:\n",
        "        predictions.append(model.word_translation(english_word))\n",
        "    print(\"\\n Word Accuracy\",(np.sum(np.asarray(predictions) == np.array(tamil)) / len(predictions)))\n",
        "    df = pd.DataFrame()\n",
        "    df[\"inputs\"] = english\n",
        "    df[\"targets\"] = tamil\n",
        "    df[\"predictions\"] = predictions\n",
        "    df.to_csv(\"/content/drive/MyDrive/Assignment3/save_outputs.csv\")\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = besttest(emb_dimension=256, neuron=256, dropout=0.2, no_of_layers=3, layer=\"LSTM\", att=False, teacher_forcing=1.0)"
      ],
      "metadata": {
        "id": "K3-20_cjBZaD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled13.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}